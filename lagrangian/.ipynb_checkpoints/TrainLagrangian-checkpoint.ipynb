{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "d4a4573e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "from jax.experimental.ode import odeint\n",
    "import matplotlib.pyplot as plt\n",
    "import pickle\n",
    "from functools import partial # reduces arguments to function by making some subset implicit\n",
    "\n",
    "from jax.experimental import stax\n",
    "from jax.experimental import optimizers\n",
    "\n",
    "# visualization\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib.patches import Circle\n",
    "from moviepy.editor import ImageSequenceClip\n",
    "from functools import partial\n",
    "import proglog\n",
    "from PIL import Image\n",
    "\n",
    "from simulate_data import generate_train_ideal\n",
    "from simulate_data import generate_train_noisy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "6188c132",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "generating data...\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'x_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_3369230/45116203.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxt_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxt_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgenerate_train_ideal\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/cs152/cs152fa21/physinet.io/lagrangian/simulate_data.py\u001b[0m in \u001b[0;36mgenerate_train_ideal\u001b[0;34m()\u001b[0m\n\u001b[1;32m    131\u001b[0m     \u001b[0;31m# x_train are the actual dynamics, xt_train are the time derivatives, and y_train are the next steps\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    132\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxt_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxt_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 133\u001b[0;31m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    134\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    135\u001b[0m \u001b[0;31m# x_0 is some random starting sequence\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'x_train' is not defined"
     ]
    }
   ],
   "source": [
    "x_train, xt_train, y_train, x_test, xt_test, y_test = generate_train_ideal()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "cdea95be",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = jax.device_put(jax.vmap(normalize_dp)(x_train))\n",
    "y_train = jax.device_put(y_train)\n",
    "\n",
    "x_test = jax.device_put(jax.vmap(normalize_dp)(x_test))\n",
    "y_test = jax.device_put(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b70d9894",
   "metadata": {},
   "outputs": [],
   "source": [
    "@jax.jit\n",
    "def update_timestep(i, opt_state, batch):\n",
    "  params = get_params(opt_state)\n",
    "  return opt_update(i, jax.grad(loss)(params, batch, time_step), opt_state)\n",
    "\n",
    "@jax.jit\n",
    "def update_derivative(i, opt_state, batch):\n",
    "  params = get_params(opt_state)\n",
    "  return opt_update(i, jax.grad(loss)(params, batch, None), opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ce102cad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the lagrangian with a parameteric model\n",
    "def learned_lagrangian(params):\n",
    "  def lagrangian(q, q_t):\n",
    "    assert q.shape == (2,)\n",
    "    state = normalize_dp(jnp.concatenate([q, q_t]))\n",
    "    return jnp.squeeze(nn_forward_fn(params, state), axis=-1)\n",
    "  return lagrangian\n",
    "\n",
    "# define the loss of the model (MSE between predicted q, \\dot q and targets)\n",
    "@jax.jit\n",
    "def loss(params, batch, time_step=None):\n",
    "  state, targets = batch\n",
    "  if time_step is not None:\n",
    "    f = partial(equation_of_motion, learned_lagrangian(params))\n",
    "    preds = jax.vmap(partial(rk4_step, f, t=0.0, h=time_step))(state)\n",
    "  else:\n",
    "    preds = jax.vmap(partial(equation_of_motion, learned_lagrangian(params)))(state)\n",
    "  return jnp.mean((preds - targets) ** 2)\n",
    "\n",
    "# build a neural network model\n",
    "init_random_params, nn_forward_fn = stax.serial(\n",
    "    stax.Dense(128),\n",
    "    stax.Softplus,\n",
    "    stax.Dense(128),\n",
    "    stax.Softplus,\n",
    "    stax.Dense(1),\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b937803e",
   "metadata": {},
   "source": [
    "# TRAIN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "11f0c0fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=0, train_loss=1843.291260, test_loss=419.025330\n",
      "iteration=1000, train_loss=92.478256, test_loss=97.354805\n",
      "iteration=2000, train_loss=46.534706, test_loss=47.283524\n",
      "iteration=3000, train_loss=18.885002, test_loss=19.809271\n",
      "iteration=4000, train_loss=8.617194, test_loss=9.475868\n",
      "iteration=5000, train_loss=4.817916, test_loss=5.193524\n",
      "iteration=6000, train_loss=3.129249, test_loss=3.440415\n",
      "iteration=7000, train_loss=2.377301, test_loss=2.603726\n",
      "iteration=8000, train_loss=1.881568, test_loss=2.102041\n",
      "iteration=9000, train_loss=1.699257, test_loss=1.914169\n",
      "iteration=10000, train_loss=2.542636, test_loss=2.703139\n",
      "iteration=11000, train_loss=2.442232, test_loss=2.419973\n",
      "iteration=12000, train_loss=1.684646, test_loss=1.734027\n",
      "iteration=13000, train_loss=0.957029, test_loss=1.106312\n",
      "iteration=14000, train_loss=0.870045, test_loss=1.014449\n",
      "iteration=15000, train_loss=0.905709, test_loss=1.050234\n",
      "iteration=16000, train_loss=1.157810, test_loss=1.220662\n",
      "iteration=17000, train_loss=0.670459, test_loss=0.766151\n",
      "iteration=18000, train_loss=0.705949, test_loss=0.827987\n",
      "iteration=19000, train_loss=0.646608, test_loss=0.724355\n",
      "iteration=20000, train_loss=0.411613, test_loss=0.520413\n",
      "iteration=21000, train_loss=1.896085, test_loss=1.635486\n",
      "iteration=22000, train_loss=1.238856, test_loss=1.167255\n",
      "iteration=23000, train_loss=0.290037, test_loss=0.364253\n",
      "iteration=24000, train_loss=0.339373, test_loss=0.402346\n",
      "iteration=25000, train_loss=1.940037, test_loss=1.633538\n",
      "iteration=26000, train_loss=0.270057, test_loss=0.337646\n",
      "iteration=27000, train_loss=0.195666, test_loss=0.272390\n",
      "iteration=28000, train_loss=0.256676, test_loss=0.304762\n",
      "iteration=29000, train_loss=0.194640, test_loss=0.265389\n",
      "iteration=30000, train_loss=0.515297, test_loss=0.510974\n",
      "iteration=31000, train_loss=0.200984, test_loss=0.249332\n",
      "iteration=32000, train_loss=0.610162, test_loss=0.582500\n",
      "iteration=33000, train_loss=0.350630, test_loss=0.372673\n",
      "iteration=34000, train_loss=0.194124, test_loss=0.220858\n",
      "iteration=35000, train_loss=0.319892, test_loss=0.325990\n",
      "iteration=36000, train_loss=0.138555, test_loss=0.170117\n",
      "iteration=37000, train_loss=0.195593, test_loss=0.203972\n",
      "iteration=38000, train_loss=0.153376, test_loss=0.177210\n",
      "iteration=39000, train_loss=0.073935, test_loss=0.108645\n",
      "iteration=40000, train_loss=0.134551, test_loss=0.161157\n",
      "iteration=41000, train_loss=0.071231, test_loss=0.096460\n",
      "iteration=42000, train_loss=0.069012, test_loss=0.095124\n",
      "iteration=43000, train_loss=0.057675, test_loss=0.081512\n",
      "iteration=44000, train_loss=0.050924, test_loss=0.075930\n",
      "iteration=45000, train_loss=0.276417, test_loss=0.268601\n",
      "iteration=46000, train_loss=0.100477, test_loss=0.113627\n",
      "iteration=47000, train_loss=0.046824, test_loss=0.065140\n",
      "iteration=48000, train_loss=0.139074, test_loss=0.140127\n",
      "iteration=49000, train_loss=0.071082, test_loss=0.083666\n",
      "iteration=50000, train_loss=0.235880, test_loss=0.219490\n",
      "iteration=51000, train_loss=0.028392, test_loss=0.044591\n",
      "iteration=52000, train_loss=0.025773, test_loss=0.040053\n",
      "iteration=53000, train_loss=0.021851, test_loss=0.034534\n",
      "iteration=54000, train_loss=0.036807, test_loss=0.044649\n",
      "iteration=55000, train_loss=0.024235, test_loss=0.033430\n",
      "iteration=56000, train_loss=0.017907, test_loss=0.027685\n",
      "iteration=57000, train_loss=0.015930, test_loss=0.025212\n",
      "iteration=58000, train_loss=0.033627, test_loss=0.039244\n",
      "iteration=59000, train_loss=0.015050, test_loss=0.023554\n",
      "iteration=60000, train_loss=0.013569, test_loss=0.021661\n",
      "iteration=61000, train_loss=0.012924, test_loss=0.020657\n",
      "iteration=62000, train_loss=0.012399, test_loss=0.019964\n",
      "iteration=63000, train_loss=0.020055, test_loss=0.026502\n",
      "iteration=64000, train_loss=0.013109, test_loss=0.019605\n",
      "iteration=65000, train_loss=0.026183, test_loss=0.030727\n",
      "iteration=66000, train_loss=0.022064, test_loss=0.028502\n",
      "iteration=67000, train_loss=0.010608, test_loss=0.017297\n",
      "iteration=68000, train_loss=0.010382, test_loss=0.016718\n",
      "iteration=69000, train_loss=0.009806, test_loss=0.016239\n",
      "iteration=70000, train_loss=0.012091, test_loss=0.017617\n",
      "iteration=71000, train_loss=0.009485, test_loss=0.015532\n",
      "iteration=72000, train_loss=0.010017, test_loss=0.015764\n",
      "iteration=73000, train_loss=0.008783, test_loss=0.014724\n",
      "iteration=74000, train_loss=0.008635, test_loss=0.014465\n",
      "iteration=75000, train_loss=0.009014, test_loss=0.014709\n",
      "iteration=76000, train_loss=0.008170, test_loss=0.013915\n",
      "iteration=77000, train_loss=0.008067, test_loss=0.013694\n",
      "iteration=78000, train_loss=0.007920, test_loss=0.013488\n",
      "iteration=79000, train_loss=0.046875, test_loss=0.046236\n",
      "iteration=80000, train_loss=0.007522, test_loss=0.013047\n",
      "iteration=81000, train_loss=0.010677, test_loss=0.015325\n",
      "iteration=82000, train_loss=0.007652, test_loss=0.013090\n",
      "iteration=83000, train_loss=0.007898, test_loss=0.013333\n",
      "iteration=84000, train_loss=0.012772, test_loss=0.018028\n",
      "iteration=85000, train_loss=0.091850, test_loss=0.085018\n",
      "iteration=86000, train_loss=0.007114, test_loss=0.012370\n",
      "iteration=87000, train_loss=0.006602, test_loss=0.011861\n",
      "iteration=88000, train_loss=0.008206, test_loss=0.013394\n",
      "iteration=89000, train_loss=0.010017, test_loss=0.015113\n",
      "iteration=90000, train_loss=0.009430, test_loss=0.014381\n",
      "iteration=91000, train_loss=0.010937, test_loss=0.015838\n",
      "iteration=92000, train_loss=0.006786, test_loss=0.011588\n",
      "iteration=93000, train_loss=0.006337, test_loss=0.011192\n",
      "iteration=94000, train_loss=0.009574, test_loss=0.013803\n",
      "iteration=95000, train_loss=0.006221, test_loss=0.010991\n",
      "iteration=96000, train_loss=0.007449, test_loss=0.012390\n",
      "iteration=97000, train_loss=0.006815, test_loss=0.011218\n",
      "iteration=98000, train_loss=0.007088, test_loss=0.011950\n",
      "iteration=99000, train_loss=0.007640, test_loss=0.012559\n",
      "iteration=100000, train_loss=0.006292, test_loss=0.011318\n",
      "iteration=101000, train_loss=0.005327, test_loss=0.010036\n",
      "iteration=102000, train_loss=0.005164, test_loss=0.009756\n",
      "iteration=103000, train_loss=0.005011, test_loss=0.009517\n",
      "iteration=104000, train_loss=0.005529, test_loss=0.009912\n",
      "iteration=105000, train_loss=0.004734, test_loss=0.009048\n",
      "iteration=106000, train_loss=0.004621, test_loss=0.008847\n",
      "iteration=107000, train_loss=0.004713, test_loss=0.008764\n",
      "iteration=108000, train_loss=0.004416, test_loss=0.008529\n",
      "iteration=109000, train_loss=0.004330, test_loss=0.008369\n",
      "iteration=110000, train_loss=0.004251, test_loss=0.008202\n",
      "iteration=111000, train_loss=0.004168, test_loss=0.008120\n",
      "iteration=112000, train_loss=0.004096, test_loss=0.007942\n",
      "iteration=113000, train_loss=0.006039, test_loss=0.009325\n",
      "iteration=114000, train_loss=0.003955, test_loss=0.007699\n",
      "iteration=115000, train_loss=0.003859, test_loss=0.007589\n",
      "iteration=116000, train_loss=0.003796, test_loss=0.007483\n",
      "iteration=117000, train_loss=0.003736, test_loss=0.007390\n",
      "iteration=118000, train_loss=0.003686, test_loss=0.007275\n",
      "iteration=119000, train_loss=0.003661, test_loss=0.007256\n",
      "iteration=120000, train_loss=0.003653, test_loss=0.007119\n",
      "iteration=121000, train_loss=0.003524, test_loss=0.007041\n",
      "iteration=122000, train_loss=0.003463, test_loss=0.006923\n",
      "iteration=123000, train_loss=0.008991, test_loss=0.011205\n",
      "iteration=124000, train_loss=0.004013, test_loss=0.007190\n",
      "iteration=125000, train_loss=0.003350, test_loss=0.006736\n",
      "iteration=126000, train_loss=0.003429, test_loss=0.006728\n",
      "iteration=127000, train_loss=0.010357, test_loss=0.012990\n",
      "iteration=128000, train_loss=0.003846, test_loss=0.007149\n",
      "iteration=129000, train_loss=0.003207, test_loss=0.006476\n",
      "iteration=130000, train_loss=0.005878, test_loss=0.008410\n",
      "iteration=131000, train_loss=0.003084, test_loss=0.006267\n",
      "iteration=132000, train_loss=0.003226, test_loss=0.006434\n",
      "iteration=133000, train_loss=0.003016, test_loss=0.006146\n",
      "iteration=134000, train_loss=0.003050, test_loss=0.006109\n",
      "iteration=135000, train_loss=0.002947, test_loss=0.006036\n",
      "iteration=136000, train_loss=0.002924, test_loss=0.005968\n",
      "iteration=137000, train_loss=0.002890, test_loss=0.005928\n",
      "iteration=138000, train_loss=0.003546, test_loss=0.006417\n",
      "iteration=139000, train_loss=0.002829, test_loss=0.005825\n",
      "iteration=140000, train_loss=0.002801, test_loss=0.005777\n",
      "iteration=141000, train_loss=0.002788, test_loss=0.005730\n",
      "iteration=142000, train_loss=0.033636, test_loss=0.033061\n",
      "iteration=143000, train_loss=0.002808, test_loss=0.005751\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration=144000, train_loss=0.002694, test_loss=0.005589\n",
      "iteration=145000, train_loss=0.002678, test_loss=0.005562\n",
      "iteration=146000, train_loss=0.002668, test_loss=0.005493\n",
      "iteration=147000, train_loss=0.002674, test_loss=0.005466\n",
      "iteration=148000, train_loss=0.002597, test_loss=0.005413\n",
      "iteration=149000, train_loss=0.002690, test_loss=0.005420\n",
      "iteration=150000, train_loss=0.003505, test_loss=0.006019\n",
      "CPU times: user 5min 10s, sys: 1min 34s, total: 6min 45s\n",
      "Wall time: 6min 9s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "\n",
    "rng = jax.random.PRNGKey(0)\n",
    "_, init_params = init_random_params(rng, (-1, 4))\n",
    "\n",
    "# numbers in comments denote stephan's settings\n",
    "batch_size = 100\n",
    "test_every = 10\n",
    "num_batches = 1500\n",
    "\n",
    "train_losses = []\n",
    "test_losses = []\n",
    "\n",
    "# adam w learn rate decay\n",
    "opt_init, opt_update, get_params = optimizers.adam(\n",
    "    lambda t: jnp.select([t < batch_size*(num_batches//3),\n",
    "                          t < batch_size*(2*num_batches//3),\n",
    "                          t > batch_size*(2*num_batches//3)],\n",
    "                         [1e-3, 3e-4, 1e-4]))\n",
    "opt_state = opt_init(init_params)\n",
    "\n",
    "for iteration in range(batch_size*num_batches + 1):\n",
    "  if iteration % batch_size == 0:\n",
    "    params = get_params(opt_state)\n",
    "    train_loss = loss(params, (x_train, xt_train))\n",
    "    train_losses.append(train_loss)\n",
    "    test_loss = loss(params, (x_test, xt_test))\n",
    "    test_losses.append(test_loss)\n",
    "    if iteration % (batch_size*test_every) == 0:\n",
    "      print(f\"iteration={iteration}, train_loss={train_loss:.6f}, test_loss={test_loss:.6f}\")\n",
    "  opt_state = update_derivative(iteration, opt_state, (x_train, xt_train))\n",
    "\n",
    "params = get_params(opt_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "207baf0b",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_name = 'LNN_Params'\n",
    "outfile = open(file_name,'wb')\n",
    "pickle.dump(params, outfile)\n",
    "outfile.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
