{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Testing Dataset\n",
    "\n",
    "Dependencies:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "source": [
    "import os\n",
    "import shutil\n",
    "import random; random.seed(42)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from matplotlib import pyplot as plt\n",
    "import torch\n",
    "from torch import nn\n",
    "#import tensorflow as tf # tensorflow-gpu==2.0.0\n",
    "#from tensorflow.python.client import device_lib \n",
    "#print(device_lib.list_local_devices())\n",
    "#import cv2"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Directories:"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "source": [
    "# training data\n",
    "train_dir = 'Data/train_and_test_split/dpc_dataset_traintest_4_200_csv/train'\n",
    "train_dir_video = 'Data/train_and_test_split/dpc_dataset_traintest_4_200_h264/train'\n",
    "\n",
    "# test data\n",
    "test_inputs_dir = 'Data/train_and_test_split/dpc_dataset_traintest_4_200_csv/test_inputs/'\n",
    "test_targets_dir = 'Data/train_and_test_split/dpc_dataset_traintest_4_200_csv/test_targets/'\n",
    "test_targets_video = 'Data/train_and_test_split/dpc_dataset_traintest_4_200_h264/test_targets/'\n",
    "\n",
    "# validation data\n",
    "validation_inputs_dir = 'Data/train_and_test_split/dpc_dataset_traintest_4_200_csv/validation_inputs/'\n",
    "validation_targets_dir = 'tData/rain_and_test_split/dpc_dataset_traintest_4_200_csv/validation_targets/'\n",
    "validation_targets_video = 'Data/train_and_test_split/dpc_dataset_traintest_4_200_h264/validation_targets/'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Coordinate based predictions:\n",
    "### Data Transformation Functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "source": [
    "# some constants\n",
    "DEFAULT_X_RED, DEFAULT_Y_RED = (240, 240)\n",
    "\n",
    "PIXEL_DISTANCE_GREEN_TO_RED = 118 # approx. value | calculated with the Pythagorean theorem and averaged: np.sqrt((y_green-y_red)**2 + (x_green-x_red)**2)\n",
    "PIXEL_DISTANCE_BLUE_TO_GREEN = 90 # approx. value | calculated with the Pythagorean theorem and averaged: np.sqrt((y_blue-y_green)**2 + (x_blue-x_green)**2)\n",
    "\n",
    "def raw_to_pixel(l):\n",
    "    '''Convert the raw coordinates to pixel coordinates.'''\n",
    "    assert isinstance(l, list)\n",
    "    return [x/5 for x in l]\n",
    "\n",
    "\n",
    "def pixel_to_raw(l):\n",
    "    '''Convert the pixel coordinates to raw coordinates.'''\n",
    "    assert isinstance(l, list)\n",
    "    return [x*5 for x in l]\n",
    "\n",
    "\n",
    "def raw_cartesian_to_polar_angles(l):\n",
    "    '''Convert the cartesian coordinates to polar coordinates.'''\n",
    "    assert isinstance(l, list)\n",
    "    x_red, y_red, x_green, y_green, x_blue, y_blue = raw_to_pixel(l)\n",
    "\n",
    "    angle_green_red = np.arctan((y_green-y_red)/(x_green-x_red+0.001))\n",
    "    angle_blue_green = np.arctan((y_blue-y_green)/(x_blue-x_green+0.001))\n",
    "    \n",
    "    return [np.sin(angle_green_red), np.cos(angle_green_red), np.sin(angle_blue_green), np.cos(angle_blue_green)]\n",
    "\n",
    "def polar_angles_to_raw_cartesian(l):\n",
    "    '''Convert the polar coordinates back to cartesian coordinates.'''\n",
    "    assert isinstance(l, list)\n",
    "    sin_angle_green_red, cos_angle_green_red, sin_angle_blue_green, cos_angle_blue_green = l\n",
    "    \n",
    "    y_green = PIXEL_DISTANCE_GREEN_TO_RED * sin_angle_green_red + DEFAULT_Y_RED\n",
    "    x_green = PIXEL_DISTANCE_GREEN_TO_RED * cos_angle_green_red + DEFAULT_X_RED\n",
    "\n",
    "    y_blue = PIXEL_DISTANCE_BLUE_TO_GREEN * sin_angle_blue_green + y_green\n",
    "    x_blue = PIXEL_DISTANCE_BLUE_TO_GREEN * cos_angle_blue_green + x_green\n",
    "    \n",
    "    return pixel_to_raw([DEFAULT_X_RED, DEFAULT_Y_RED, x_green, y_green, x_blue, y_blue])"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Verify that the raw -> pixel conversion and pixel -> raw works as intended, and that the cartesian -> polar conversion and polar -> cartesian conversion works as intended."
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "source": [
    "raw_coordinates = list(np.array([240, 240, 357.4438349670886, 228.55685234634907, 444.41827493559794, 205.41712909467287])*5)\n",
    "pixel_coordinates = raw_to_pixel(raw_coordinates)\n",
    "new_raw_coordinates = pixel_to_raw(pixel_coordinates)\n",
    "assert raw_coordinates == new_raw_coordinates, '`Raw -> Pixel` and `Pixel -> Raw` coordinate conversion methods are malfunctioning.'\n",
    "\n",
    "raw_cartesian = list(np.array([240, 240, 357.4438349670886, 228.55685234634907, 444.41827493559794, 205.41712909467287])*5)\n",
    "polar = raw_cartesian_to_polar_angles(raw_cartesian)\n",
    "new_raw_cartesian = polar_angles_to_raw_cartesian(polar)\n",
    "assert [round(x) for x in raw_cartesian] == [round(x) for x in new_raw_cartesian], 'Cartesian to Polar and Polar to Cartesian methods are malfunctioning.'"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Data reading functions"
   ],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Parsing training data:\n",
    "training data x-y matching is like this:\n",
    "x: a list of 4 frames\n",
    "y: the frame that follows"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "source": [
    "def parse_training_annotations(csv_file):\n",
    "    '''Parse the training annotations from a CSV file.'''\n",
    "    X_data = []\n",
    "    y_data = []\n",
    "    f = pd.read_csv(csv_file, header=None, delim_whitespace=True, engine='python')\n",
    "    temp = []\n",
    "    for i, row in f.iterrows():\n",
    "        if len(temp) < 4:\n",
    "            # convert the cartesian pixel coordinates to polar coordinates\n",
    "            temp.append(raw_cartesian_to_polar_angles(row.to_list()))\n",
    "        else:\n",
    "            # the output frame\n",
    "            # convert the cartesian pixel coordinates to polar coordinates\n",
    "            next_frame = raw_cartesian_to_polar_angles(row.to_list())\n",
    "\n",
    "            # save\n",
    "            X_data.append(temp.copy())\n",
    "            y_data.append(next_frame.copy())\n",
    "\n",
    "            # add output frame to the inputs and remove the first\n",
    "            temp.pop(0)\n",
    "            temp.append(next_frame)\n",
    "    return X_data, y_data"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load in data"
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "source": [
    "BATCH_SIZE = 4000\n",
    "\n",
    "# load in all separate files\n",
    "X = []\n",
    "y = []\n",
    "for filename in tqdm([x for x in os.listdir(train_dir) if not x.startswith('.')]):\n",
    "    # load in a file\n",
    "    X_data, y_data = parse_training_annotations(os.path.join(train_dir, filename))\n",
    "\n",
    "    X = X + X_data\n",
    "    y = y + y_data"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 40/40 [01:00<00:00,  1.51s/it]\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "source": [
    "class DoublePendulumDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,X_list,y_list):\n",
    "        self.sample_list = list(zip(X_list, y_list))\n",
    "    \n",
    "    def __getitem__(self,index):\n",
    "        X_sample,y_sample = self.sample_list[index]\n",
    "        return X_sample,y_sample\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self.sample_list)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "myDataSet = DoublePendulumDataset(X,y)\n",
    "myDataLoader = torch.utils.data.DataLoader(myDataSet,batch_size=4000)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self):\n",
    "        # We want a model of 4 layer LSTM with 32 features output, and a dense layer to form the 4 feature output.\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        # Defining some parameters\n",
    "        self.hidden_size = 32\n",
    "        self.n_layers = 4\n",
    "\n",
    "        #Defining the layers\n",
    "        # LSTM layer\n",
    "        self.lstm = nn.LSTM(input_size = 4, hidden_size = 32, num_layers = 4, batch_first = True)\n",
    "\n",
    "        # Fully connected layer\n",
    "        self.fc = nn.Linear(32, 4)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        #h0 = torch.zeros(self.n_layers, x.size(0), self.hidden_size).requires_grad_()\n",
    "        #c0 = torch.zeros(self.n_layers, x.size(0), self.hidden_size).requires_grad_()\n",
    "        out= self.lstm(x) # (h0.detach(), c0.detach())\n",
    "        out = out[:, -1, :]\n",
    "        out = self.fc(out)\n",
    "\n",
    "        return out"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "class Optimization:\n",
    "    def __init__(self, model, loss_fn, optimizer):\n",
    "        self.model = model\n",
    "        self.loss_fn = loss_fn\n",
    "        self.optimizer = optimizer\n",
    "        self.train_losses = []\n",
    "        self.val_losses = []\n",
    "    \n",
    "    def train_step(self, x, y):\n",
    "        # Sets model to train mode\n",
    "        self.model.train()\n",
    "\n",
    "        # Makes predictions\n",
    "        yhat = self.model(x)\n",
    "\n",
    "        # Computes loss\n",
    "        loss = self.loss_fn(y, yhat)\n",
    "\n",
    "        # Computes gradients\n",
    "        loss.backward()\n",
    "\n",
    "        # Updates parameters and zeroes gradients\n",
    "        self.optimizer.step()\n",
    "        self.optimizer.zero_grad()\n",
    "\n",
    "        # Returns the loss\n",
    "        return loss.item()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "source": [
    "# Instantiate the model with hyperparameters\n",
    "model = Model()\n",
    "\n",
    "# Define hyperparameters\n",
    "n_epochs = 10\n",
    "lr=0.01\n",
    "\n",
    "# Define Loss, Optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=lr)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "for epoch in range(2):  # loop over the dataset multiple times\n",
    "\n",
    "    running_loss = 0.0\n",
    "    for i, data in enumerate(myDataLoader, 0):\n",
    "        # get the inputs; data is a list of [inputs, labels]\n",
    "        inputs, labels = data\n",
    "\n",
    "        # zero the parameter gradients\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "        # forward + backward + optimize\n",
    "        outputs = model(torch.Tensor(inputs))\n",
    "        loss = criterion(outputs,labels)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # print statistics\n",
    "        running_loss += loss.item()\n",
    "        if i % 2000 == 1999:    # print every 2000 mini-batches\n",
    "            print('[%d, %5d] loss: %.3f' %\n",
    "                  (epoch + 1, i + 1, running_loss / 2000))\n",
    "            running_loss = 0.0\n",
    "\n",
    "print('Finished Training')"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "ValueError",
     "evalue": "only one element tensors can be converted to Python scalars",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1367565/4166245808.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;31m# get the inputs; data is a list of [inputs, labels]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m         \u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlabels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdata\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m         \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTensor\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m         \u001b[0;31m# zero the parameter gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mValueError\u001b[0m: only one element tensors can be converted to Python scalars"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "source": [
    "\n",
    "for epoch in range(1, n_epochs + 1):\n",
    "    optimizer.zero_grad() # Clears existing gradients from previous epoch\n",
    "    output = model(torch_X)\n",
    "    loss = criterion(output.view(-1), torch_y.view(-1))\n",
    "    loss.backward() # Does backpropagation and calculates gradients\n",
    "    optimizer.step() # Updates the weights accordingly\n",
    "    \n",
    "    if epoch%1 == 0:\n",
    "        print('Epoch: {}/{}.............'.format(epoch, n_epochs), end=' ')\n",
    "        print(\"Loss: {:.4f}\".format(loss.item()))"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "NameError",
     "evalue": "name 'torch_X' is not defined",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1367565/1478591267.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_epochs\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0moptimizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mzero_grad\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Clears existing gradients from previous epoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m     \u001b[0moutput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtorch_X\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m     \u001b[0mloss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcriterion\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0moutput\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtorch_y\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m     \u001b[0mloss\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m# Does backpropagation and calculates gradients\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'torch_X' is not defined"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "source": [
    "def plot_history(history):\n",
    "    \"\"\"Plot the training process of a model.\"\"\"\n",
    "    \n",
    "    hist = pd.DataFrame(history.history)\n",
    "    hist['epoch'] = history.epoch\n",
    "\n",
    "    plt.figure()\n",
    "    plt.xlabel('Epoch') \n",
    "    plt.ylabel('Mean Squared Error [MSE]')\n",
    "    plt.plot(hist['epoch'], hist['mean_squared_error'].tolist(),\n",
    "           label='Train Error')\n",
    "    plt.legend()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "source": [
    "def parse_testing_input(csv_file):\n",
    "    '''Parse the training annotations from a CSV file.'''\n",
    "    X_data = []\n",
    "    f = pd.read_csv(csv_file, header=None, delim_whitespace=True, engine='python')\n",
    "    temp = []\n",
    "    for i, row in f.iterrows():\n",
    "            temp.append(raw_cartesian_to_polar_angles(row.to_list()))\n",
    "    X_data.append(temp.copy())\n",
    "    return X_data\n",
    "\n",
    "def parse_testing_target(csv_file):\n",
    "    '''Parse the training annotations from a CSV file.'''\n",
    "    Y_data = []\n",
    "    f = pd.read_csv(csv_file, header=None, delim_whitespace=True, engine='python')\n",
    "    for i, row in f.iterrows():\n",
    "        Y_data.append(raw_cartesian_to_polar_angles(row.to_list()))\n",
    "        break\n",
    "    return Y_data\n",
    "\n",
    "# load in all separate files\n",
    "X_test = []\n",
    "y_test = []\n",
    "for filename in tqdm([x for x in os.listdir(test_inputs_dir) if not x.startswith('.')]):\n",
    "    # load in a file\n",
    "    X_data_test=parse_testing_input(os.path.join(test_inputs_dir, filename))\n",
    "    y_data_test=parse_testing_target(os.path.join(test_targets_dir, filename))\n",
    "    \n",
    "    X_test.append(X_data_test)\n",
    "    y_test.append(y_data_test)\n",
    "\n",
    "num_batches = len(X_test)\n",
    "num_records = num_batches * BATCH_SIZE\n",
    "print(f'{num_records} training records spread over {num_batches} batches of size {BATCH_SIZE}')"
   ],
   "outputs": [
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "100%|██████████| 60/60 [00:00<00:00, 164.45it/s]"
     ]
    },
    {
     "output_type": "stream",
     "name": "stdout",
     "text": [
      "240000 training records spread over 60 batches of size 4000\n"
     ]
    },
    {
     "output_type": "stream",
     "name": "stderr",
     "text": [
      "\n"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "source": [
    "X_test = np.array(X_test)\n",
    "y_test = np.array(y_test)\n",
    "# torch_X has n batches of size 4000, each sample is 4 frame, each frame is 4 value (sin,cos,sin,cos), e.g. torch.Size([69,4000,4,4])\n",
    "torch_X_test = torch.from_numpy(X_test)\n",
    "# torch_y has n batches of size 4000, each sample is a sequence of frames, unknown length, of 4 values, e.g. torch.Size([69,4000,4])\n",
    "torch_y_test = torch.from_numpy(y_test)\n",
    "torch_X_test = torch_X_test.view(-1,4,4).float()\n",
    "torch_y_test = torch_y_test.view(-1,4).float()"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "source": [
    "test_output = model(torch_X_test)"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "source": [
    "torch_y_test"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[-1.0000,  0.0034,  0.5655,  0.8248],\n",
       "        [-0.3178,  0.9482, -0.8746,  0.4849],\n",
       "        [-0.3807,  0.9247,  0.8750,  0.4841],\n",
       "        [ 0.7917,  0.6109,  0.4247,  0.9053],\n",
       "        [-0.9556,  0.2948,  0.4437,  0.8962],\n",
       "        [ 0.7754,  0.6315,  0.5439,  0.8392],\n",
       "        [-0.9744,  0.2249, -0.8289,  0.5593],\n",
       "        [-0.0356,  0.9994, -0.6706,  0.7419],\n",
       "        [-0.8248,  0.5654,  0.6440,  0.7650],\n",
       "        [ 0.9940,  0.1091,  0.4647,  0.8854],\n",
       "        [-0.2819,  0.9594,  0.1268,  0.9919],\n",
       "        [-0.7264,  0.6873,  0.9998,  0.0177],\n",
       "        [ 0.5000,  0.8660, -0.9518,  0.3069],\n",
       "        [-0.3801,  0.9249,  0.2331,  0.9724],\n",
       "        [-0.9950,  0.1004, -0.9136,  0.4066],\n",
       "        [-0.0492,  0.9988, -0.3036,  0.9528],\n",
       "        [ 0.2725,  0.9622, -0.8770,  0.4806],\n",
       "        [ 0.9936,  0.1127,  0.2890,  0.9573],\n",
       "        [ 0.0915,  0.9958, -0.3649,  0.9310],\n",
       "        [-0.3807,  0.9247, -0.6947,  0.7193],\n",
       "        [-0.5027,  0.8645,  0.0777,  0.9970],\n",
       "        [-0.9351,  0.3543, -0.9243,  0.3818],\n",
       "        [ 0.3805,  0.9248, -0.9541,  0.2995],\n",
       "        [-0.1019,  0.9948, -0.6242,  0.7813],\n",
       "        [-0.9699,  0.2433, -0.2115,  0.9774],\n",
       "        [ 0.2932,  0.9560,  0.9799,  0.1995],\n",
       "        [ 0.1630,  0.9866,  0.8643,  0.5031],\n",
       "        [ 0.6172,  0.7868,  0.7115,  0.7027],\n",
       "        [ 0.3152,  0.9490,  0.7512,  0.6601],\n",
       "        [-0.8390,  0.5441,  0.9621,  0.2727],\n",
       "        [ 0.8150,  0.5795, -0.9988,  0.0487],\n",
       "        [-0.2305,  0.9731,  0.4176,  0.9086],\n",
       "        [ 0.6417,  0.7670, -0.3348,  0.9423],\n",
       "        [-0.2140,  0.9768,  0.9074,  0.4203],\n",
       "        [ 0.9945,  0.1049, -0.4160,  0.9094],\n",
       "        [ 0.9900,  0.1407, -0.9935,  0.1136],\n",
       "        [-0.2700,  0.9629, -0.8706,  0.4921],\n",
       "        [-0.8772,  0.4802,  0.9260,  0.3775],\n",
       "        [ 0.9231,  0.3845,  0.9993,  0.0378],\n",
       "        [-0.3532,  0.9356, -0.9572,  0.2894],\n",
       "        [-0.4587,  0.8886, -0.9773,  0.2120],\n",
       "        [-0.2585,  0.9660,  0.6612,  0.7502],\n",
       "        [-0.9972,  0.0750, -0.0467,  0.9989],\n",
       "        [-0.7289,  0.6846, -0.6312,  0.7756],\n",
       "        [ 0.9525,  0.3045,  0.9916,  0.1293],\n",
       "        [ 0.6697,  0.7426,  0.9462,  0.3235],\n",
       "        [ 0.9346,  0.3556, -0.4832,  0.8755],\n",
       "        [-0.6572,  0.7537, -0.9984,  0.0566],\n",
       "        [ 0.8367,  0.5477,  0.9965,  0.0839],\n",
       "        [-0.3106,  0.9505,  0.9730,  0.2310],\n",
       "        [ 0.8310,  0.5562, -0.7859,  0.6183],\n",
       "        [ 0.2033,  0.9791, -0.7684,  0.6400],\n",
       "        [ 0.9984,  0.0559, -0.8255,  0.5644],\n",
       "        [-0.3851,  0.9229, -0.2457,  0.9694],\n",
       "        [-0.3294,  0.9442,  0.0841,  0.9965],\n",
       "        [-0.1611,  0.9869, -0.7447,  0.6674],\n",
       "        [-0.9880,  0.1542,  0.7203,  0.6936],\n",
       "        [ 0.3631,  0.9318,  0.6917,  0.7222],\n",
       "        [ 0.0102,  0.9999,  0.4954,  0.8687],\n",
       "        [-0.3178,  0.9482,  0.9044,  0.4266]])"
      ]
     },
     "metadata": {},
     "execution_count": 28
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "source": [
    "test_output"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 0.1009,  0.5713, -0.0491,  0.5025],\n",
       "        [ 0.1049,  0.5731, -0.0550,  0.5052],\n",
       "        [ 0.1076,  0.5541, -0.0351,  0.4866],\n",
       "        [ 0.1159,  0.5373, -0.0329,  0.4772],\n",
       "        [ 0.1013,  0.5712, -0.0485,  0.5021],\n",
       "        [ 0.1157,  0.5392, -0.0337,  0.4785],\n",
       "        [ 0.1002,  0.5793, -0.0578,  0.5103],\n",
       "        [ 0.1084,  0.5672, -0.0519,  0.5006],\n",
       "        [ 0.1039,  0.5657, -0.0434,  0.4966],\n",
       "        [ 0.1160,  0.5352, -0.0344,  0.4772],\n",
       "        [ 0.1060,  0.5688, -0.0477,  0.4997],\n",
       "        [ 0.1026,  0.5730, -0.0533,  0.5047],\n",
       "        [ 0.1109,  0.5583, -0.0496,  0.4951],\n",
       "        [ 0.1057,  0.5690, -0.0477,  0.4998],\n",
       "        [ 0.1000,  0.5779, -0.0575,  0.5095],\n",
       "        [ 0.1076,  0.5689, -0.0512,  0.5012],\n",
       "        [ 0.1098,  0.5629, -0.0509,  0.4979],\n",
       "        [ 0.1162,  0.5333, -0.0330,  0.4757],\n",
       "        [ 0.1080,  0.5683, -0.0514,  0.5009],\n",
       "        [ 0.1040,  0.5764, -0.0556,  0.5071],\n",
       "        [ 0.1040,  0.5748, -0.0524,  0.5050],\n",
       "        [ 0.1006,  0.5798, -0.0578,  0.5103],\n",
       "        [ 0.1105,  0.5589, -0.0498,  0.4956],\n",
       "        [ 0.1065,  0.5717, -0.0535,  0.5036],\n",
       "        [ 0.1003,  0.5789, -0.0558,  0.5092],\n",
       "        [ 0.1134,  0.5408, -0.0298,  0.4772],\n",
       "        [ 0.1131,  0.5457, -0.0337,  0.4815],\n",
       "        [ 0.1152,  0.5385, -0.0312,  0.4768],\n",
       "        [ 0.1136,  0.5450, -0.0340,  0.4813],\n",
       "        [ 0.1048,  0.5557, -0.0357,  0.4880],\n",
       "        [ 0.1157,  0.5303, -0.0285,  0.4716],\n",
       "        [ 0.1085,  0.5618, -0.0432,  0.4940],\n",
       "        [ 0.1138,  0.5500, -0.0425,  0.4878],\n",
       "        [ 0.1101,  0.5488, -0.0327,  0.4826],\n",
       "        [ 0.1146,  0.5422, -0.0412,  0.4839],\n",
       "        [ 0.1134,  0.5449, -0.0456,  0.4873],\n",
       "        [ 0.1046,  0.5753, -0.0552,  0.5063],\n",
       "        [ 0.1042,  0.5582, -0.0375,  0.4902],\n",
       "        [ 0.1164,  0.5279, -0.0268,  0.4697],\n",
       "        [ 0.1043,  0.5740, -0.0553,  0.5058],\n",
       "        [ 0.1039,  0.5732, -0.0550,  0.5053],\n",
       "        [ 0.1096,  0.5528, -0.0358,  0.4861],\n",
       "        [ 0.1002,  0.5773, -0.0546,  0.5079],\n",
       "        [ 0.1021,  0.5788, -0.0559,  0.5088],\n",
       "        [ 0.1165,  0.5262, -0.0267,  0.4690],\n",
       "        [ 0.1156,  0.5354, -0.0295,  0.4745],\n",
       "        [ 0.1140,  0.5477, -0.0443,  0.4877],\n",
       "        [ 0.1053,  0.5557, -0.0369,  0.4884],\n",
       "        [ 0.1134,  0.5471, -0.0441,  0.4873],\n",
       "        [ 0.1087,  0.5526, -0.0345,  0.4854],\n",
       "        [ 0.1130,  0.5512, -0.0471,  0.4907],\n",
       "        [ 0.1087,  0.5658, -0.0519,  0.4999],\n",
       "        [ 0.1133,  0.5445, -0.0456,  0.4871],\n",
       "        [ 0.1048,  0.5746, -0.0535,  0.5053],\n",
       "        [ 0.1062,  0.5700, -0.0495,  0.5010],\n",
       "        [ 0.1070,  0.5703, -0.0533,  0.5028],\n",
       "        [ 0.1024,  0.5617, -0.0410,  0.4939],\n",
       "        [ 0.1138,  0.5442, -0.0338,  0.4808],\n",
       "        [ 0.1097,  0.5601, -0.0431,  0.4930],\n",
       "        [ 0.1085,  0.5532, -0.0348,  0.4859]], grad_fn=<AddmmBackward>)"
      ]
     },
     "metadata": {},
     "execution_count": 30
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "source": [
    "diff = (torch_y_test-test_output) / torch_y_test"
   ],
   "outputs": [],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "source": [
    "diff.double()"
   ],
   "outputs": [
    {
     "output_type": "execute_result",
     "data": {
      "text/plain": [
       "tensor([[ 1.1009e+00, -1.6596e+02,  1.0868e+00,  3.9077e-01],\n",
       "        [ 1.3300e+00,  3.9554e-01,  9.3708e-01, -4.1950e-02],\n",
       "        [ 1.2826e+00,  4.0072e-01,  1.0401e+00, -5.0896e-03],\n",
       "        [ 8.5356e-01,  1.2045e-01,  1.0775e+00,  4.7289e-01],\n",
       "        [ 1.1060e+00, -9.3762e-01,  1.1094e+00,  4.3976e-01],\n",
       "        [ 8.5080e-01,  1.4618e-01,  1.0620e+00,  4.2984e-01],\n",
       "        [ 1.1029e+00, -1.5763e+00,  9.3023e-01,  8.7726e-02],\n",
       "        [ 4.0410e+00,  4.3242e-01,  9.2260e-01,  3.2521e-01],\n",
       "        [ 1.1260e+00, -4.8345e-04,  1.0674e+00,  3.5090e-01],\n",
       "        [ 8.8330e-01, -3.9047e+00,  1.0740e+00,  4.6107e-01],\n",
       "        [ 1.3759e+00,  4.0721e-01,  1.3763e+00,  4.9626e-01],\n",
       "        [ 1.1412e+00,  1.6636e-01,  1.0533e+00, -2.7539e+01],\n",
       "        [ 7.7815e-01,  3.5534e-01,  9.4793e-01, -6.1347e-01],\n",
       "        [ 1.2781e+00,  3.8479e-01,  1.2046e+00,  4.8599e-01],\n",
       "        [ 1.1005e+00, -4.7587e+00,  9.3702e-01, -2.5326e-01],\n",
       "        [ 3.1886e+00,  4.3037e-01,  8.3122e-01,  4.7401e-01],\n",
       "        [ 5.9698e-01,  4.1498e-01,  9.4200e-01, -3.6140e-02],\n",
       "        [ 8.8307e-01, -3.7335e+00,  1.1144e+00,  5.0314e-01],\n",
       "        [-1.8100e-01,  4.2926e-01,  8.5911e-01,  4.6200e-01],\n",
       "        [ 1.2732e+00,  3.7668e-01,  9.2004e-01,  2.9494e-01],\n",
       "        [ 1.2069e+00,  3.3502e-01,  1.6746e+00,  4.9345e-01],\n",
       "        [ 1.1076e+00, -6.3641e-01,  9.3750e-01, -3.3674e-01],\n",
       "        [ 7.0956e-01,  3.9564e-01,  9.4778e-01, -6.5433e-01],\n",
       "        [ 2.0455e+00,  4.2534e-01,  9.1427e-01,  3.5541e-01],\n",
       "        [ 1.1034e+00, -1.3787e+00,  7.3614e-01,  4.7906e-01],\n",
       "        [ 6.1346e-01,  4.3435e-01,  1.0304e+00, -1.3918e+00],\n",
       "        [ 3.0627e-01,  4.4692e-01,  1.0390e+00,  4.2887e-02],\n",
       "        [ 8.1341e-01,  3.1563e-01,  1.0439e+00,  3.2139e-01],\n",
       "        [ 6.3965e-01,  4.2570e-01,  1.0452e+00,  2.7086e-01],\n",
       "        [ 1.1249e+00, -2.1386e-02,  1.0371e+00, -7.8976e-01],\n",
       "        [ 8.5804e-01,  8.4855e-02,  9.7152e-01, -8.6762e+00],\n",
       "        [ 1.4704e+00,  4.2269e-01,  1.1034e+00,  4.5633e-01],\n",
       "        [ 8.2261e-01,  2.8291e-01,  8.7314e-01,  4.8232e-01],\n",
       "        [ 1.5144e+00,  4.3821e-01,  1.0360e+00, -1.4819e-01],\n",
       "        [ 8.8481e-01, -4.1701e+00,  9.0091e-01,  4.6787e-01],\n",
       "        [ 8.8547e-01, -2.8724e+00,  9.5407e-01, -3.2896e+00],\n",
       "        [ 1.3874e+00,  4.0251e-01,  9.3660e-01, -2.9001e-02],\n",
       "        [ 1.1188e+00, -1.6256e-01,  1.0405e+00, -2.9839e-01],\n",
       "        [ 8.7389e-01, -3.7294e-01,  1.0269e+00, -1.1412e+01],\n",
       "        [ 1.2953e+00,  3.8650e-01,  9.4219e-01, -7.4785e-01],\n",
       "        [ 1.2265e+00,  3.5497e-01,  9.4376e-01, -1.3839e+00],\n",
       "        [ 1.4240e+00,  4.2774e-01,  1.0541e+00,  3.5203e-01],\n",
       "        [ 1.1005e+00, -6.6964e+00, -1.6852e-01,  4.9153e-01],\n",
       "        [ 1.1400e+00,  1.5452e-01,  9.1145e-01,  3.4405e-01],\n",
       "        [ 8.7771e-01, -7.2784e-01,  1.0270e+00, -2.6282e+00],\n",
       "        [ 8.2740e-01,  2.7900e-01,  1.0311e+00, -4.6679e-01],\n",
       "        [ 8.7799e-01, -5.4017e-01,  9.0838e-01,  4.4290e-01],\n",
       "        [ 1.1603e+00,  2.6267e-01,  9.6300e-01, -7.6349e+00],\n",
       "        [ 8.6448e-01,  9.7548e-04,  1.0442e+00, -4.8043e+00],\n",
       "        [ 1.3499e+00,  4.1860e-01,  1.0354e+00, -1.1014e+00],\n",
       "        [ 8.6404e-01,  9.0139e-03,  9.4012e-01,  2.0640e-01],\n",
       "        [ 4.6504e-01,  4.2216e-01,  9.3242e-01,  2.1891e-01],\n",
       "        [ 8.8649e-01, -8.7485e+00,  9.4479e-01,  1.3704e-01],\n",
       "        [ 1.2721e+00,  3.7734e-01,  7.8234e-01,  4.7876e-01],\n",
       "        [ 1.3225e+00,  3.9635e-01,  1.5883e+00,  4.9717e-01],\n",
       "        [ 1.6644e+00,  4.2220e-01,  9.2849e-01,  2.4661e-01],\n",
       "        [ 1.1037e+00, -2.6426e+00,  1.0569e+00,  2.8799e-01],\n",
       "        [ 6.8649e-01,  4.1590e-01,  1.0488e+00,  3.3423e-01],\n",
       "        [-9.7911e+00,  4.3985e-01,  1.0871e+00,  4.3243e-01],\n",
       "        [ 1.3413e+00,  4.1661e-01,  1.0385e+00, -1.3886e-01]],\n",
       "       dtype=torch.float64, grad_fn=<CopyBackwards>)"
      ]
     },
     "metadata": {},
     "execution_count": 24
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "source": [
    "plot_history(test_output)"
   ],
   "outputs": [
    {
     "output_type": "error",
     "ename": "AttributeError",
     "evalue": "'Tensor' object has no attribute 'history'",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m/tmp/ipykernel_1364008/699961418.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mplot_history\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_output\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/tmp/ipykernel_1364008/801008108.py\u001b[0m in \u001b[0;36mplot_history\u001b[0;34m(history)\u001b[0m\n\u001b[1;32m      2\u001b[0m     \u001b[0;34m\"\"\"Plot the training process of a model.\"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m     \u001b[0mhist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDataFrame\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      5\u001b[0m     \u001b[0mhist\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epoch'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'Tensor' object has no attribute 'history'"
     ]
    }
   ],
   "metadata": {}
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "source": [],
   "outputs": [],
   "metadata": {}
  }
 ],
 "metadata": {
  "orig_nbformat": 4,
  "language_info": {
   "name": "python",
   "version": "3.8.5",
   "mimetype": "text/x-python",
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "pygments_lexer": "ipython3",
   "nbconvert_exporter": "python",
   "file_extension": ".py"
  },
  "kernelspec": {
   "name": "python3",
   "display_name": "Python 3.8.5 64-bit ('cs152': conda)"
  },
  "interpreter": {
   "hash": "9950993fed0fd9aed4b27a856c953e397ab7787bcf7b4a95697366d9a811f382"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}